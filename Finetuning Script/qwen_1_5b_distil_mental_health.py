# -*- coding: utf-8 -*-
"""Qwen_1.5B_Distil.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wrqI6i_78JbFFIGPBPy-pO6udFyRQxhQ
"""

from google.colab import files

uploaded = files.upload()

# !pip install torch transformers datasets peft accelerate bitsandbytes trl safetensors ipywidgets huggingface_hub python-dotenv --upgrade

#Loading the Model and Tokenizer
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
device_map = {'':0}
import torch
#bitsandbytes parameters

#Activate 4-bit precision base model loading
use_4bit = True
#Compute dtype for 4 bit base models
bnb_4bit_compute_dtype = 'float16'
#Quantization type
bnb_4bit_quant_type = 'nf4'
#Activate double quantization??
use_double_nested_quant = False
# BitsAndBytesConfig int-4 config

compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=use_4bit,
    bnb_4bit_use_double_quant=use_double_nested_quant,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype
)

model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

# Load the pretrained model
model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, use_cache = False, device_map=device_map)
model.config.pretraining_tp = 1

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

from datasets import load_dataset

# Load the JSON dataset
dataset = load_dataset('json', data_files='./fine_tuning_dataset.json', split='train')

def prompt_template(instruction, output):
  text = f'''
  {tokenizer.bos_token}You are a converstional chatbot assistant. Your job is to answer queries asked by the user in an empathetic manner. Be brief with you answers. Do not try to repeat them.
  <｜User｜> {instruction} <|Assistant|> {output}{tokenizer.eos_token}
  '''
  return text

dataset = dataset.map(lambda x:{'text': prompt_template(x['instruction'], x['output'])})
dataset = dataset.remove_columns(['instruction', 'output'])

# Function to find linear layers
from torch import nn
def find_linear_layers(model):
    linear_layers = []
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            linear_layers.append(name)
        elif "Conv1D" in str(type(module)):
            linear_layers.append(name)
    return linear_layers

#Lora Configuration for PEFT
lora_r = 64
lora_alpha = 16
lora_dropout = 0.1

from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM

# LoRA config based on QLoRA paper
peft_config = LoraConfig(
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        r=lora_r,
        bias="none",
        target_modules = find_linear_layers(model),
        task_type="CAUSAL_LM",
)

from transformers import TrainingArguments
output_dir = 'Qlora-finetuned-model'
#Number of training epochs
num_train_epochs = 1
#Enable fp16
fp16 = True
#Batch size per training
per_device_train_batch_size = 2
#Number of update steps to accumulate the gradients for
gradient_accumulation_steps = 2
#Enable gradient checkpoint
gradient_checkpointing = True
#Gradient Clipping
max_grad_norm = 0.3
#Learning Rate
learning_rate = 2e-4
#Weight decay
weight_decay = 0.01
#Optimizer to use
optim = 'paged_adamw_32bit'
#Learning rate schedule
lr_scheduler_type = 'cosine'
# Ratio of steps for a linear warmup (from 0 to learning rate)
warmup_ratio = 0.03
# Group sequences into batches with same length
# Saves memory and speeds up training considerably
group_by_length = False
# Save checkpoint every X updates steps
save_steps = 0
# Log every X updates steps
logging_steps = 100
# Disable tqdm
disable_tqdm= False
# SFTTrainer parameters
# Maximum sequence length to use
max_seq_length = 1024 #None
# Pack multiple short examples in the same input sequence to increase efficiency
packing = True #False


# Define the training arguments
args = TrainingArguments(
    output_dir="./FineTuned-Qwen-1.5-Distil-Instruct-Model",
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size, # 6 if use_flash_attention else 4,
    gradient_accumulation_steps=gradient_accumulation_steps,
    gradient_checkpointing=gradient_checkpointing,
    optim=optim,
    logging_steps=logging_steps,
    save_strategy="epoch",
    learning_rate=learning_rate,
    weight_decay=weight_decay,
    fp16=fp16,
    max_grad_norm=max_grad_norm,
    warmup_ratio=warmup_ratio,
    group_by_length=group_by_length,
    lr_scheduler_type=lr_scheduler_type,
    disable_tqdm=disable_tqdm,
    report_to="tensorboard",
    seed=42
)

from trl import SFTTrainer, SFTConfig


sft_config = SFTConfig(
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    warmup_steps=500,
    logging_dir='./logs',
    output_dir='./model_output',
    save_total_limit=2,
    save_steps=1000,
    logging_steps=500,
    fp16=True  # Enable if using compatible hardware
)

# Initialize SFTTrainer
trainer = SFTTrainer(
    model = model,
    peft_config = peft_config,
    train_dataset=dataset,
    processing_class=tokenizer,
    args=args,
)

from google.colab import output
output.enable_custom_widget_manager()

trainer.train()

from transformers import AutoModelForCausalLM

# Load the base model in FP16 (no quantization)
base_model = AutoModelForCausalLM.from_pretrained(
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    torch_dtype="float16",  # Load in FP16
    device_map="auto"       # Use GPU if available
)

from peft import PeftModel, PeftConfig
lora_weights_path = './FineTuned-Qwen-1.5-Distil-Instruct-Model/checkpoint-1628'
# Load LoRA weights
lora_model = PeftModel.from_pretrained(base_model, lora_weights_path)

# Merge LoRA weights with the base model
lora_model = lora_model.merge_and_unload()

from transformers import AutoTokenizer, TextStreamer, AutoModelForCausalLM
from peft import PeftModel
# Set up the TextStreamer
streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

# Define the input prompt
#input_prompt = "I am very sad. What should i do?"
input_prompt = f'''{tokenizer.bos_token} You are a converstional chatbot assistant. Your job is to answer queries asked by the user in an empathetic manner. Be brief with you answers. Do not try to repeat them.
  <｜User｜> I am feeling sad. What should i do? <|Assistant|> '''
# Tokenize the input
#input_prompt = formatting_func(input_prompt)
input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids.to('cuda')

# Generate text with streaming
lora_model.generate(
    input_ids=input_ids,
    max_new_tokens=200,
    streamer=streamer,
    temperature=0.7,  # Adjust for creativity
    top_k=50,# Adjust for focused outputs
    eos_token_id=[tokenizer.eos_token_id]
)

lora_model.push_to_hub("Aspect05/Qwen-1.5-Distil-FP16-Updated", token = "HUGGING_FACE_TOKEN") # Online saving
tokenizer.push_to_hub("Aspect05/Qwen-1.5-Distil-FP16-Updated", token = "HUGGING_FACE_TOKEN") # Online saving

